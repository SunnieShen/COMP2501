---
title: "COMP2501 Lab Sheet 2"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instructions

* You are expected to answer the questions in the sheet in class
* You are encouraged to try different code or arguments
* You can discuss with other students for ideas but do not copy from others
* If you have any questions, ask Google, the TAs or lecturer
* Sample answers will be given after the class on Moodle

------------------------------------------------------------------------

### Environmental setup

You need to have the `tidyverse`, `caret`, `dslabs`, `lubridate` and `HistData` packages installed. If not yet, please run `install.packages(c("tidyverse", "caret", "dslabs", "lubridate", "HistData"))` in your R environment. Noting that if you have installed the `tidyverse` package, `dplyr` and `ggplot2` are installed by default.

```{r}
# Install the packages.
# install.packages(c("tidyverse", "caret", "dslabs", "lubridate", "HistData"))
# Load the packages.
library(tidyverse)
library(caret)
library(dslabs)
library(lubridate)
library(HistData)
```

### 1. Practice with the dataset `reported_heights`.

#### a. Load the dataset `reported_heights`. Print the dimension and the first 6 rows of the `reported_heights` dataset, respectively.

```{r}
library(dslabs)
data("reported_heights")
dim(reported_heights)
head(reported_heights)

```

#### b. Preprocess the dataset `reported_heights` with the following sub-questions step by step. Create a dataset copy of `reported_heights` named `dat`. 1) Add a new column `date_time` with the standard date&time format using `ymd_hms()` function of `lubridate` package to the `dat` dataset according to the `time_stamp` column. 2) Filter the `dat` dataset and just preserve the observations whose date is between `2016-01-25` and `2016-02-01`. (hint: `make_date(2016, 01, 25) <= date_time < make_date(2016, 02, 01)`. 3) Add a new column `type` to the `dat` dataset. If the observation happened between 08:15:00 and 08:30:59 on 2016-01-25, the `type` would be `inclass`, otherwise `online`. 4) Select just the `sex`, `height` and `type` columns to preserve. 5) Print the dimension and the first 6 rows of the `dat` dataset, respectively.

```{r}
# mutate==pd.assign()
library(tidyverse)
library(lubridate)
dat <- reported_heights |> 
  mutate(date_time = ymd_hms(time_stamp)) |> 
  filter(date_time >= make_date(2016, 01, 25) & date_time < make_date(2016, 02, 01)) |> 
  mutate(type = ifelse(day(date_time) == 25 & 
                       hour(date_time) == 8 & 
                       between(minute(date_time), 15, 30), 
                       "inclass", "online")) |> 
  select(sex, height, type)
dim(dat)
head(dat)
```

#### c. Guessing the sex of each observation of the `dat` dataset. Assign `dat$type` to `x`, and `dat$sex` to `y`. Randomly splitting the data into training and test sets with a ratio of `0.5:0.5` using `createDataPartition()` funciton of `caret` package. Create the `y_hat` (y prediction) with the random `sample()` function and compute the overall accuracy of this method on the test set. Please fix the random seed to 2025.

```{r}
library(caret)
set.seed(2025)
x <- dat$type
y <- factor(dat$sex, c("Female", "Male"))
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- dat[test_index, ]
train_set <- dat[-test_index, ]
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE) |> 
          factor(levels = levels(factor(test_set$sex)))
mean(y_hat == test_set$sex)

```

#### d. Similar to above, at this time, predicting the sex of each observation of the `dat` dataset with a heuristic rule: if the `type` is equal to `inclass`, the `sex` would be predicted as `Female`, otherwise `Male`. Define the `y_hat` and compute the overall accuracy of this method on test set. Please fix the random seed to 2025. 

```{r}
library(caret)
set.seed(2025)
y_hat <- ifelse(test_set$type == "inclass", "Female", "Male") |> 
          factor(levels = levels(factor(test_set$sex)))
mean(y_hat == test_set$sex)

```

#### e. Following above, show the confusion matrix as `cm`, and output the overall accuracy, sensitivity, specificity according to the confusion matrix. At the same time, use `sensitivity()`, `specificity()` and `F_meas()` to compute the sensitivity, specificity and F1 score, respectively.  

```{r}
cm <- confusionMatrix(data = y_hat, reference = factor(test_set$sex))
cm
cm$overall["Accuracy"]
cm$byClass[c("Sensitivity","Specificity", "Prevalence")]

sensitivity(data = y_hat, reference = factor(test_set$sex))
specificity(data = y_hat, reference = factor(test_set$sex))
F_meas(data = y_hat, reference = factor(test_set$sex))

```


### 2. Practice more with the dataset `galton_heights`.

#### a. Load the dataset `GaltonFamilies` from the `HistData` package as `galton_heights`. Print the dimension, the variable names and the first 6 rows of the `galton_heights` dataset, respectively.

```{r}
library(HistData)

galton_heights <- GaltonFamilies

dim(galton_heights)
names(galton_heights)
head(galton_heights)

```

#### b. Preprocess the dataset `galton_heights` for subsequent analysis of father and son heights with the following sub-questions step by step. Create a dataset copy of `galton_heights` named `heights_father_son`. 1) Filter the `heights_father_son` dataset and just preserve the observations whose gender is `Male`. 2) Group the `heights_father_son` dataset by `family` and sample just one observation for each family. 3) Ungroup the `heights_father_son` dataset and select just the `father` and `childHeight` columns to preserve, and rename the column `childHeight` to `son`. 5) Print the dimension and the first 6 rows of the `heights_father_son` dataset, respectively.

```{r}
library(tidyverse)
heights_father_son <- galton_heights |>
  filter(gender == "male") |>
  group_by(family) |>
  sample_n(1) |>
  ungroup() |>
  select(father, childHeight) |>
  rename(son = childHeight)

dim(heights_father_son)
head(heights_father_son)

```

#### c. Guessing the son's height with the average height of sons of the `heights_father_son` training set. Assign `heights_father_son$father` to `x`, and `heights_father_son$son` to `y`. Randomly splitting the data into training and test sets with a ratio of `0.5:0.5` using `createDataPartition()` funciton of `caret` package. Compute the root mean squared error of this method on test set. Please fix the random seed to 1234.

```{r}
library(caret)
set.seed(1234)
x <- heights_father_son$father
y <- heights_father_son$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- heights_father_son[test_index, ]
train_set <- heights_father_son[-test_index, ]

guess_son <- mean(train_set$son)
guess_son
sqrt(mean((guess_son - test_set$son)^2))

```

#### d. Similar to above, at this time, predicting the son's height of the `heights_father_son` dataset with the least squares estimates between the father's and son's height by using `lm()` function. Construct the model as `fit` using the training set and print the estimated model coefficients with `fit$coef`. Define the `y_hat` as `fit$coef[1] + fit$coef[2]*test_set$father` and compute the root mean squared error of this method on test set. Please fix the random seed to 1234. 

```{r}
library(caret)
set.seed(1234)
fit <- lm(son ~ father, data = train_set)
fit$coef
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
sqrt(mean((y_hat - test_set$son)^2))

```

#### e. Following above, define the `y_hat` by using `predict()` function at this time. Compute the root mean squared error of this method on test set and observe whether it is consistent with the above. Please fix the random seed to 1234.

```{r}
library(caret)
set.seed(1234)
y_hat <- predict(fit, test_set)
sqrt(mean((y_hat - test_set$son)^2))

```


### 3. Practice more with the dataset `mnist`.

#### a. Load the dataset `mnist_27`. Print the data type (`class()`) and the names of each element (`names()`) of the `mnist_27` dataset, respectively.

```{r}
library(dslabs)
data("mnist_27")
class(mnist_27)
names(mnist_27)

```

#### b. Use the `mnist_27` training set to build a model with the following models respectively from the caret package. Noting that you might need to install some packages for some of the models. Now train all the models with all the default parameters and compute the overall accuracy for each model on the test set. And find out the highest accuracy and its corresponding model in this case.

```{r}
library(caret)
models_list <- c("glm", "lda", "naive_bayes",  "svmLinear", "knn", "rf", "mlp")

ml_mnist_27 <- function(ml_model){
  train_model <- train(y ~ ., method = ml_model, data = mnist_27$train)
  y_hat_model <- predict(train_model, mnist_27$test, type = "raw")
  confusionMatrix(y_hat_model, mnist_27$test$y)$overall[["Accuracy"]]
}

acc_mnist_27 <- sapply(models_list, ml_mnist_27)
print(acc_mnist_27)
print(max(acc_mnist_27))
print(which.max(acc_mnist_27))

# train_glm <- train(y ~ ., method = "glm", data = mnist_27$train)
# y_hat_glm <- predict(train_glm, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_glm, mnist_27$test$y)$overall[["Accuracy"]]
# 
# train_lda <- train(y ~ ., method = "lda", data = mnist_27$train)
# y_hat_lda <- predict(train_lda, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_lda, mnist_27$test$y)$overall[["Accuracy"]]
# 
# train_nb <- train(y ~ ., method = "naive_bayes", data = mnist_27$train)
# y_hat_nb <- predict(train_nb, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_nb, mnist_27$test$y)$overall[["Accuracy"]]
# 
# train_svm <- train(y ~ ., method = "svmLinear", data = mnist_27$train)
# y_hat_svm <- predict(train_svm, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_svm, mnist_27$test$y)$overall[["Accuracy"]]
# 
# train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)
# y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[["Accuracy"]]
# 
# train_rf <- train(y ~ ., method = "rf", data = mnist_27$train)
# y_hat_rf <- predict(train_rf, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_rf, mnist_27$test$y)$overall[["Accuracy"]]
# 
# train_mlp <- train(y ~ ., method = "mlp", data = mnist_27$train)
# y_hat_mlp <- predict(train_mlp, mnist_27$test, type = "raw")
# confusionMatrix(y_hat_mlp, mnist_27$test$y)$overall[["Accuracy"]]

```

#### c. Load the dataset `mnist` with `read_mnist()`. Print the data type (`class()`) and the names of each element (`names()`) of the `mnist` dataset, respectively. Additionally, print the dimension of `mnist$train$images` and `mnist$test$images`.

```{r}
library(dslabs)
mnist <- read_mnist()
class(mnist)
names(mnist)
dim(mnist$train$images)
dim(mnist$test$images)

```

#### d. Use the subset of `mnist` dataset to build a model with the `knn` model from the caret package. Firstly sample 3,000 random rows from the training set and 500 random rows from the test set. Please fix the random seed to 2025. Now train the `knn` model with the default parameters and compute the overall accuracy on the test set. Noting that you don't need to  preprocess the images here and remember to add column names to the training and test sets.

```{r}
library(caret)
set.seed(2025)
index_train <- sample(nrow(mnist$train$images), 3000)
x_train <- mnist$train$images[index_train,]
y_train <- factor(mnist$train$labels[index_train])

index_test <- sample(nrow(mnist$test$images), 500)
x_test <- mnist$test$images[index_test,]
y_test <- factor(mnist$test$labels[index_test])

# library(matrixStats)
# sds <- colSds(x_train)
# qplot(sds, bins = 256)
# 
# nzv <- nearZeroVar(x_train)
# image(matrix(1:784 %in% nzv, 28, 28))
# 
# col_index <- setdiff(1:ncol(x_train), nzv)
# length(col_index)
# 
# colnames(x_train) <- 1:ncol(mnist$train$images)
# colnames(x_test) <- colnames(x_train)

colnames(x_train) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x_train)

train_knn <- train(x_train, y_train, method = "knn")
y_hat_knn <- predict(train_knn, x_test, type = "raw")
confusionMatrix(y_hat_knn, y_test)$overall[["Accuracy"]]

```

#### e. Following above, use the `trainControl()` function to perform 10-fold cross validation and use the `tuneGrid` parameter with `k = seq(3, 21, 2)`. Show the results of the cross validation using the `ggplot` function, and highlight the max. Then find out the best k and best performing model, and compute the overall accuracy for the best performing model on the test set.

```{r}
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(x_train, y_train, method = "knn", 
                   tuneGrid = data.frame(k = seq(3, 21, 2)),
                   trControl = control)
ggplot(train_knn_cv, highlight = TRUE)

k_best <- train_knn_cv$bestTune
k_best
train_knn_best <- train_knn_cv$finalModel
train_knn_best
y_hat_knn <- predict(train_knn, x_test, type = "raw")
confusionMatrix(y_hat_knn, y_test)$overall[["Accuracy"]]

```
